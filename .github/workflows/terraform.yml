# Add a short, numbered name for roadmap clarity
name: "02 - Terraform Infrastructure"
on:
  workflow_call:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        type: string
      action:
        description: 'Terraform action to perform'
        required: false
        type: string
        default: 'plan'
      allow-local-firewall:
        description: 'Allow local firewall rule for SQL Server (act testing only)'
        required: false
        type: boolean
        default: false
      enable-debug:
        description: 'Enable verbose debug logging'
        required: false
        type: boolean
        default: false
    outputs:
      api_app_name:
        description: "The name of the API App Service"
        value: ${{ jobs.terraform.outputs.api_app_name }}
      react_app_name:
        description: "The name of the React App Service"
        value: ${{ jobs.terraform.outputs.react_app_name }}
      acr_registry_url:
        description: "The URL of the Azure Container Registry"
        value: ${{ jobs.terraform.outputs.acr_registry_url }}
      resource_group_name:
        description: "The name of the Azure Resource Group"
        value: ${{ jobs.terraform.outputs.resource_group_name }}
    secrets:
      AZURE_CLIENT_ID:
        required: true
        description: 'Azure Client ID for OIDC authentication'
      AZURE_TENANT_ID:
        required: true
        description: 'Azure Tenant ID for OIDC authentication'
      AZURE_SUBSCRIPTION_ID:
        required: true
        description: 'Azure Subscription ID'
      ARM_CLIENT_ID:
        required: true
        description: 'Service Principal Client ID for Terraform'
      ARM_CLIENT_SECRET:
        required: true
        description: 'Service Principal Client Secret for Terraform'
      ARM_TENANT_ID:
        required: true
        description: 'Service Principal Tenant ID for Terraform'
      SQL_ADMIN_USERNAME:
        required: true
        description: 'SQL Server admin username'
      SQL_ADMIN_PASSWORD:
        required: true
        description: 'SQL Server admin password'
      AZURE_CREDENTIALS:
        required: false
        description: 'JSON credentials for service principal (used only with act) - optional'


env:
  WORKING_DIRECTORY: './infra/terraform'

jobs:
  terraform:
    name: "Terraform apply"
    runs-on: ubuntu-latest
    container:
      image: ${{ vars.ACTIONS_RUNNER_IMAGE || 'ubuntu:latest' }}
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    environment: ${{ inputs.environment }}
    timeout-minutes: 60

    outputs:
      api_app_name: ${{ steps.outputs.outputs.api_app_name }}
      react_app_name: ${{ steps.outputs.outputs.react_app_name }}
      acr_registry_url: ${{ steps.outputs.outputs.acr_registry_url }}
      resource_group_name: ${{ steps.outputs.outputs.resource_group_name }}

    permissions:
      contents: read
      id-token: write

    concurrency:
      group: terraform-${{ inputs.environment }}
      cancel-in-progress: false

    env:
      # ARM_* variables are for Terraform Azure provider service principal authentication
      ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
      # Terraform variables - all passed via environment to avoid committing .tfvars with secrets
      TF_VAR_environment: ${{ inputs.environment }}
      TF_VAR_subscription_id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      TF_VAR_sql_admin_username: ${{ secrets.SQL_ADMIN_USERNAME }}
      TF_VAR_sql_admin_password: ${{ secrets.SQL_ADMIN_PASSWORD }}
      TF_VAR_enable_local_firewall: ${{ inputs.allow-local-firewall }}
      # Non-sensitive configuration variables (from GitHub Variables)
      # Environment-specific (from GitHub Environment Variables)
      TF_VAR_region: ${{ vars.AZURE_REGION || 'canadacentral' }}
      TF_VAR_resource_group_name: ${{ vars.AZURE_RESOURCE_GROUP_NAME || 'consilient-resource-group' }}
      # Container App Environment configuration (from GitHub Repository Variables)
      TF_VAR_create_container_app_environment: ${{ vars.CAE_CREATE_NEW || 'false' }}
      TF_VAR_use_shared_container_environment: ${{ vars.CAE_USE_SHARED || 'true' }}
      TF_VAR_shared_container_environment_name: ${{ vars.CAE_SHARED_NAME || 'consilient-env' }}
      TF_VAR_container_app_environment_name_template: ${{ vars.CAE_NAME_TEMPLATE || 'consilient-cae-{environment}' }}

    defaults:
      run:
        working-directory: ${{ env.WORKING_DIRECTORY }}  # All steps run in ./infra/terraform unless explicitly overridden

    steps:
      - name: Checkout code
        uses: actions/checkout@v4.2.2
        with:
          fetch-depth: 1
        # Note: Checkout runs in root directory (./), composite action default overrides job default
        # MUST run before custom actions that reference ./.github/actions/

      - name: Validate Inputs
        uses: ./.github/actions/validate-inputs
        with:
          environment: ${{ inputs.environment }}
          action: ${{ inputs.action }}

      - name: Validate Required Secrets
        shell: bash
        run: |
          echo "=== Validating Required Secrets ==="
          MISSING_SECRETS=()

          # OIDC Authentication Secrets (for azure-login)
          if [ -z "${{ secrets.AZURE_CLIENT_ID }}" ]; then
            MISSING_SECRETS+=("AZURE_CLIENT_ID (OIDC authentication)")
          fi
          if [ -z "${{ secrets.AZURE_TENANT_ID }}" ]; then
            MISSING_SECRETS+=("AZURE_TENANT_ID (OIDC authentication)")
          fi
          if [ -z "${{ secrets.AZURE_SUBSCRIPTION_ID }}" ]; then
            MISSING_SECRETS+=("AZURE_SUBSCRIPTION_ID (Azure subscription)")
          fi

          # Terraform Provider Secrets (service principal)
          if [ -z "${{ secrets.ARM_CLIENT_ID }}" ]; then
            MISSING_SECRETS+=("ARM_CLIENT_ID (Terraform provider auth)")
          fi
          if [ -z "${{ secrets.ARM_CLIENT_SECRET }}" ]; then
            MISSING_SECRETS+=("ARM_CLIENT_SECRET (Terraform provider auth)")
          fi
          if [ -z "${{ secrets.ARM_TENANT_ID }}" ]; then
            MISSING_SECRETS+=("ARM_TENANT_ID (Terraform provider auth)")
          fi

          # SQL Server Secrets (for Terraform)
          if [ -z "${{ secrets.SQL_ADMIN_USERNAME }}" ]; then
            MISSING_SECRETS+=("SQL_ADMIN_USERNAME (SQL server creation)")
          fi
          if [ -z "${{ secrets.SQL_ADMIN_PASSWORD }}" ]; then
            MISSING_SECRETS+=("SQL_ADMIN_PASSWORD (SQL server creation)")
          fi

          # Act Testing Fallback (optional - only warn)
          if [ -z "${{ secrets.AZURE_CREDENTIALS }}" ]; then
            echo "‚ÑπÔ∏è  Info: AZURE_CREDENTIALS not set (only required for local 'act' testing)"
          fi

          # Report missing secrets
          if [ ${#MISSING_SECRETS[@]} -gt 0 ]; then
            echo ""
            echo "‚ùå ERROR: Missing required secrets:"
            for secret in "${MISSING_SECRETS[@]}"; do
              echo "   - $secret"
            done
            echo ""
            echo "Please configure these secrets in GitHub repository settings."
            echo ""
            echo "Secret Categories:"
            echo "  OIDC Authentication: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_SUBSCRIPTION_ID"
            echo "  Terraform Provider: ARM_CLIENT_ID, ARM_CLIENT_SECRET, ARM_TENANT_ID"
            echo "  SQL Server: SQL_ADMIN_USERNAME, SQL_ADMIN_PASSWORD"
            echo "  Act Testing (optional): AZURE_CREDENTIALS"
            exit 1
          fi

          echo "‚úÖ All required secrets are configured"

      - name: Login to Azure (OIDC + act fallback)
        # AZURE_CLIENT_ID is for OIDC authentication (production)
        # ARM_CLIENT_ID (from secrets) is used by Terraform for service principal auth
        uses: ./.github/actions/azure-login
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          azure-credentials: ${{ secrets.AZURE_CREDENTIALS || '{}' }}

      - name: Terraform Format Check
        id: fmt
        # Runs in ./infra/terraform (job default)
        run: |
          if ! terraform fmt -check -recursive; then
            echo "‚ö†Ô∏è Terraform formatting issues detected"
            if [ -z "$ACT" ]; then
              echo "::error::Terraform files need formatting. Run 'terraform fmt -recursive' to fix."
              exit 1
            else
              echo "Skipping format check failure in act environment (local testing)"
            fi
          else
            echo "‚úÖ Terraform formatting check passed"
          fi

      - name: Debug Environment Variables
        if: inputs.enable-debug == true || runner.debug == '1'
        uses: ./.github/actions/debug-variables
        with:
          section-title: 'Terraform Configuration'
          variables: |
            {
              "TF_VAR_environment": "${{ env.TF_VAR_environment }}",
              "TF_VAR_subscription_id": "${{ env.TF_VAR_subscription_id }}",
              "TF_VAR_region": "${{ env.TF_VAR_region }}",
              "TF_VAR_resource_group_name": "${{ env.TF_VAR_resource_group_name }}",
              "AZURE_REGION": "${{ vars.AZURE_REGION || 'not set (using default)' }}",
              "AZURE_RESOURCE_GROUP_NAME": "${{ vars.AZURE_RESOURCE_GROUP_NAME || 'not set (using default)' }}",
              "CAE_CREATE_NEW": "${{ vars.CAE_CREATE_NEW || 'not set (using default)' }}",
              "CAE_USE_SHARED": "${{ vars.CAE_USE_SHARED || 'not set (using default)' }}"
            }

      - name: Configure Local Firewall Variable
        run: |
          # Override TF_VAR_enable_local_firewall based on ACT environment
          # ACT is set automatically by the act tool when running locally
          if [ -n "$ACT" ] && [ "${{ inputs.allow-local-firewall }}" = "true" ]; then
            echo "TF_VAR_enable_local_firewall=true" >> $GITHUB_ENV
            echo "‚úÖ Local firewall rule ENABLED (act detected)"
          else
            echo "TF_VAR_enable_local_firewall=false" >> $GITHUB_ENV
            echo "‚ÑπÔ∏è Local firewall rule disabled"
          fi

      - name: Setup Terraform (if needed for non-container environments)
        if: false  # Currently uses container image with Terraform pre-installed
        uses: hashicorp/setup-terraform@de84b05c27089f6cc8b3c38e5ddbe9dc3fd7e7b9  # v3.0.0
        with:
          terraform_version: ${{ vars.TERRAFORM_VERSION || '1.6.0' }}

      - name: Terraform Init
        id: init
        # Runs in ./infra/terraform (job default)
        run: terraform init

      - name: Build Resource Group ID
        shell: bash
        run: |
          echo ""
          echo "=== Building Resource Group ID ==="
          echo ""

          echo "Environment: ${TF_VAR_environment}"
          echo "Region: ${TF_VAR_region}"
          echo "Resource Group: ${TF_VAR_resource_group_name}"

          # Build RG_ID for later use in resource imports
          RG_ID="/subscriptions/${TF_VAR_subscription_id}/resourceGroups/${TF_VAR_resource_group_name}"
          echo "Resource Group ID: ${RG_ID}"
          echo "RG_ID=${RG_ID}" >> $GITHUB_ENV

      - name: Detect and Import Existing Resources
        id: import
        shell: bash
        run: |
          set +e  # Don't exit on error, we'll handle errors ourselves

          # Check if state file exists - skip imports if this is a fresh deployment
          if [ ! -f "terraform.tfstate" ] && [ ! -f ".terraform/terraform.tfstate" ]; then
            echo "‚ÑπÔ∏è  No Terraform state file found - this appears to be a fresh deployment"
            echo "‚ÑπÔ∏è  Skipping resource import step (will create all resources from scratch)"
            exit 0
          fi

          echo "=== Step 1: Reading CAE Configuration from Environment Variables ==="

          # CAE configuration from environment variables
          USE_SHARED_CAE="${TF_VAR_use_shared_container_environment}"
          SHARED_CAE_NAME="${TF_VAR_shared_container_environment_name}"
          CREATE_CAE="${TF_VAR_create_container_app_environment}"

          echo "Configuration from environment:"
          echo "  use_shared_container_environment: ${USE_SHARED_CAE}"
          echo "  shared_container_environment_name: ${SHARED_CAE_NAME}"
          echo "  create_container_app_environment: ${CREATE_CAE}"

          # Determine expected CAE name based on configuration
          if [ "$USE_SHARED_CAE" = "true" ]; then
            # Shared mode: Use fixed name (NO environment substitution!)
            EXPECTED_CAE_NAME="${SHARED_CAE_NAME}"
            echo "Using shared mode with FIXED name: ${EXPECTED_CAE_NAME}"
            echo "  (This name is used across ALL environments: dev, staging, prod)"
          else
            # Template mode: Would resolve from template, but we'll just let Terraform handle it
            echo "Using template mode - Terraform will resolve the name"
            EXPECTED_CAE_NAME=""  # Don't try to guess
          fi

          echo ""
          echo "‚úÖ Configuration loaded from environment variables"
          echo "All CAE settings will be managed by Terraform"

          echo ""
          echo "=== Step 2: Importing Existing Azure Resources ==="
          echo ""

          # Initialize tracking for failed imports
          failed_imports=0
          failed_critical_imports=0
          failed_import_list=()
          failed_critical_import_list=()

          # Function to import a resource if it exists
          # Second parameter can be "critical" to mark resource as essential
          import_resource() {
            local tf_address="$1"
            local resource_id="$2"
            local resource_name="$3"
            local is_critical="${4:-false}"  # Default to non-critical

            # Check if already in state
            if terraform state list | grep -q "^${tf_address}\$"; then
              echo "  ‚úÖ Already in state: ${resource_name}"
              return 0
            fi

            # Try to import
            echo "  üì• Importing: ${resource_name}"
            local import_output
            if import_output=$(terraform import "${tf_address}" "${resource_id}" 2>&1); then
              echo "  ‚úÖ Imported successfully: ${resource_name}"
              return 0
            else
              # Check if error is because resource doesn't exist
              if echo "$import_output" | grep -q "Cannot import non-existent"; then
                echo "  ‚ÑπÔ∏è  Resource does not exist in Azure: ${resource_name}"
              elif echo "$import_output" | grep -q "Resource already managed"; then
                echo "  ‚úÖ Already managed: ${resource_name}"
              else
                echo "  ‚ö†Ô∏è  Import failed: ${resource_name}"
                echo "$import_output" | grep -i "error" | head -3
                ((failed_imports++))

                if [ "$is_critical" = "critical" ]; then
                  ((failed_critical_imports++))
                  failed_critical_import_list+=("${resource_name}")
                else
                  failed_import_list+=("${resource_name}")
                fi
              fi
              return 1
            fi
          }

          echo "1. Resource Group"
          RG_ID="/subscriptions/${TF_VAR_subscription_id}/resourceGroups/${TF_VAR_resource_group_name}"
          import_resource "azurerm_resource_group.main" "${RG_ID}" "Resource Group"

          echo ""
          echo "2. Networking Resources"
          VNET_ID="${RG_ID}/providers/Microsoft.Network/virtualNetworks/consilient-vnet-${TF_VAR_environment}"
          SUBNET_ID="${VNET_ID}/subnets/consilient-subnet-${TF_VAR_environment}"
          import_resource "azurerm_virtual_network.main" "${VNET_ID}" "Virtual Network"
          import_resource "azurerm_subnet.main" "${SUBNET_ID}" "Subnet"

          echo ""
          echo "3. Container Registry"
          ACR_NAME=$(terraform output -raw acr_name 2>/dev/null || echo "consilientacr${TF_VAR_environment}$(echo -n "${TF_VAR_subscription_id}-${TF_VAR_resource_group_name}" | md5sum | cut -c1-6)")
          ACR_ID="${RG_ID}/providers/Microsoft.ContainerRegistry/registries/${ACR_NAME}"
          import_resource "azurerm_container_registry.main" "${ACR_ID}" "Container Registry" "critical"

          echo ""
          echo "4. SQL Server and Databases"
          SQL_SUFFIX=$(echo -n "${TF_VAR_subscription_id}-${TF_VAR_resource_group_name}" | md5sum | cut -c1-6)
          SQL_SERVER_NAME="consilient-sqlsrv-${TF_VAR_environment}-${SQL_SUFFIX}"
          SQL_SERVER_ID="${RG_ID}/providers/Microsoft.Sql/servers/${SQL_SERVER_NAME}"
          MAIN_DB_ID="${SQL_SERVER_ID}/databases/consilient_main_${TF_VAR_environment}"
          HANGFIRE_DB_ID="${SQL_SERVER_ID}/databases/consilient_hangfire_${TF_VAR_environment}"
          import_resource "azurerm_mssql_server.main" "${SQL_SERVER_ID}" "SQL Server" "critical"
          import_resource "module.main_db.azurerm_mssql_database.this" "${MAIN_DB_ID}" "Main Database" "critical"
          import_resource "module.hangfire_db.azurerm_mssql_database.this" "${HANGFIRE_DB_ID}" "Hangfire Database" "critical"

          echo ""
          echo "5. Storage Account and Container"
          STORAGE_NAME="consilientloki${TF_VAR_environment}${SQL_SUFFIX}"
          STORAGE_ID="${RG_ID}/providers/Microsoft.Storage/storageAccounts/${STORAGE_NAME}"
          STORAGE_CONTAINER_ID="${STORAGE_ID}/blobServices/default/containers/loki-data"
          PRIVATE_ENDPOINT_ID="${RG_ID}/providers/Microsoft.Network/privateEndpoints/consilient-pe-loki-storage-${TF_VAR_environment}"
          import_resource "azurerm_storage_account.loki" "${STORAGE_ID}" "Loki Storage Account"
          import_resource "azurerm_storage_container.loki" "${STORAGE_CONTAINER_ID}" "Loki Storage Container"
          import_resource "azurerm_private_endpoint.loki_storage" "${PRIVATE_ENDPOINT_ID}" "Loki Storage Private Endpoint"

          echo ""
          echo "6. Managed Identity"
          IDENTITY_ID="${RG_ID}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/consilient-loki-identity-${TF_VAR_environment}"
          import_resource "azurerm_user_assigned_identity.loki" "${IDENTITY_ID}" "Loki Managed Identity"

          echo ""
          echo "7. Role Assignment (Loki -> Storage)"
          # Get the principal_id of the managed identity
          PRINCIPAL_ID=$(az identity show --ids "${IDENTITY_ID}" --query principalId -o tsv 2>/dev/null || echo "")
          if [ -n "$PRINCIPAL_ID" ]; then
            echo "  Found managed identity principal ID: ${PRINCIPAL_ID}"
            # List role assignments for the storage account and find the one with this principal
            ROLE_ASSIGNMENT_ID=$(az role assignment list --scope "${STORAGE_ID}" --query "[?principalId=='${PRINCIPAL_ID}'].id | [0]" -o tsv 2>/dev/null || echo "")
            if [ -n "$ROLE_ASSIGNMENT_ID" ]; then
              echo "  Found role assignment: ${ROLE_ASSIGNMENT_ID}"
              import_resource "azurerm_role_assignment.loki_blob" "${ROLE_ASSIGNMENT_ID}" "Loki Storage Blob Role Assignment"
            else
              echo "  ‚ÑπÔ∏è  No role assignment found for this managed identity on storage account"
            fi
          else
            echo "  ‚ÑπÔ∏è  Managed identity not found, skipping role assignment import"
          fi

          echo ""
          echo "8. Container App Environment"
          # Import CAE only if:
          # 1. We're NOT creating a new one (create_container_app_environment = false)
          # 2. We're in shared mode (use_shared_container_environment = true)
          # 3. The CAE name is known (from terraform.tfvars)
          # 4. The CAE is in the SAME resource group as other resources

          if [ "$CREATE_CAE" = "false" ] && [ "$USE_SHARED_CAE" = "true" ] && [ -n "$EXPECTED_CAE_NAME" ]; then
            echo "  Shared mode detected: Checking if CAE '${EXPECTED_CAE_NAME}' exists in resource group '${TF_VAR_resource_group_name}'"

            # Check if CAE exists in Azure in the SAME resource group
            CAE_EXISTS=$(az containerapp env show \
              --name "${EXPECTED_CAE_NAME}" \
              --resource-group "${TF_VAR_resource_group_name}" \
              --query "id" -o tsv 2>/dev/null || echo "")

            if [ -n "$CAE_EXISTS" ]; then
              echo "  CAE exists in same resource group - attempting import"
              CAE_ID="${RG_ID}/providers/Microsoft.App/managedEnvironments/${EXPECTED_CAE_NAME}"
              import_resource "azurerm_container_app_environment.shared[0]" "${CAE_ID}" "Container App Environment"
            else
              echo "  ‚ÑπÔ∏è  CAE '${EXPECTED_CAE_NAME}' not found in resource group '${TF_VAR_resource_group_name}'"
              echo "  ‚ÑπÔ∏è  Terraform will use data source to reference CAE from another resource group"
            fi
          elif [ "$CREATE_CAE" = "true" ]; then
            echo "  ‚ÑπÔ∏è  create_container_app_environment=true - Terraform will create a new CAE"
            echo "  ‚ÑπÔ∏è  Attempting to import in case it already exists..."

            # Try to determine CAE name (in template mode, Terraform resolves it)
            if [ "$USE_SHARED_CAE" = "true" ]; then
              CAE_NAME="${EXPECTED_CAE_NAME}"
            else
              # Template mode - resolve the template from environment variable
              CAE_TEMPLATE="${TF_VAR_container_app_environment_name_template}"
              CAE_NAME="${CAE_TEMPLATE/\{environment\}/${TF_VAR_environment}}"
            fi

            if [ -n "$CAE_NAME" ]; then
              CAE_ID="${RG_ID}/providers/Microsoft.App/managedEnvironments/${CAE_NAME}"
              import_resource "azurerm_container_app_environment.shared[0]" "${CAE_ID}" "Container App Environment"
            fi
          else
            echo "  ‚ÑπÔ∏è  CAE configuration doesn't require import (using existing CAE via data source or ID)"
          fi

          echo ""
          echo "9. Container App (Loki)"
          CONTAINER_APP_ID="${RG_ID}/providers/Microsoft.App/containerApps/consilient-loki-${TF_VAR_environment}"
          import_resource "azurerm_container_app.loki" "${CONTAINER_APP_ID}" "Loki Container App"

          echo ""
          echo "10. Grafana"
          GRAFANA_ID="${RG_ID}/providers/Microsoft.Dashboard/grafana/consilient-grafana-${TF_VAR_environment}"
          import_resource "azurerm_dashboard_grafana.main" "${GRAFANA_ID}" "Grafana Dashboard"

          echo ""
          echo "11. App Service Plans"
          ASP_REACT_ID="${RG_ID}/providers/Microsoft.Web/serverFarms/consilient-asp-react-${TF_VAR_environment}"
          ASP_API_ID="${RG_ID}/providers/Microsoft.Web/serverFarms/consilient-asp-api-${TF_VAR_environment}"
          import_resource "module.react_app.azurerm_service_plan.this" "${ASP_REACT_ID}" "React App Service Plan" "critical"
          import_resource "module.api_app.azurerm_service_plan.this" "${ASP_API_ID}" "API App Service Plan" "critical"

          echo ""
          echo "12. App Services"
          APP_REACT_ID="${RG_ID}/providers/Microsoft.Web/sites/consilient-react-${TF_VAR_environment}"
          APP_API_ID="${RG_ID}/providers/Microsoft.Web/sites/consilient-api-${TF_VAR_environment}"
          import_resource "module.react_app.azurerm_linux_web_app.this" "${APP_REACT_ID}" "React App Service" "critical"
          import_resource "module.api_app.azurerm_linux_web_app.this" "${APP_API_ID}" "API App Service" "critical"

          echo ""
          echo "=== Import Process Complete ==="
          echo ""
          echo "Configuration will be managed by terraform.tfvars"
          echo "No workflow overrides applied"

          # Generate GitHub step summary for critical failures
          if [ $failed_critical_imports -gt 0 ]; then
            echo "### ‚ùå Critical Import Failures" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The following **critical** resources failed to import:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            for resource in "${failed_critical_import_list[@]}"; do
              echo "- \`${resource}\`" >> $GITHUB_STEP_SUMMARY
            done
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Action Required**: These resources are essential to the infrastructure and must be resolvable by Terraform." >> $GITHUB_STEP_SUMMARY
            echo "- Check if the resources actually exist in Azure under the correct resource group" >> $GITHUB_STEP_SUMMARY
            echo "- Verify the resource naming conventions match the Terraform configuration" >> $GITHUB_STEP_SUMMARY
            echo "- Ensure proper Azure credentials and permissions to list resources" >> $GITHUB_STEP_SUMMARY
            echo "- If resources don't exist yet, remove them from tfstate or use \`terraform apply\` to create them" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Generate GitHub step summary for non-critical warnings
          if [ $failed_imports -gt 0 ] && [ $failed_critical_imports -eq 0 ]; then
            echo "### ‚ö†Ô∏è Non-Critical Import Warnings" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The following non-critical resources failed to import:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            for resource in "${failed_import_list[@]}"; do
              echo "- \`${resource}\`" >> $GITHUB_STEP_SUMMARY
            done
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Note**: Import failures are expected if resources don't exist in Azure yet." >> $GITHUB_STEP_SUMMARY
            echo "Terraform will create them during the apply step." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "Total import attempts: $(terraform state list | wc -l) resources in state" >> $GITHUB_STEP_SUMMARY
          echo "Failed critical imports: $failed_critical_imports" >> $GITHUB_STEP_SUMMARY
          echo "Failed non-critical imports: ${#failed_import_list[@]}" >> $GITHUB_STEP_SUMMARY

          # Fail if critical resources failed to import
          if [ $failed_critical_imports -gt 0 ]; then
            echo ""
            echo "‚ùå FAILURE: Critical resources failed to import. Terraform state is out of sync with Azure resources."
            exit 1
          fi

          exit 0  # Succeed if only non-critical imports failed

      - name: Terraform Validate
        id: validate
        # Runs in ./infra/terraform (job default)
        run: terraform validate

      - name: Terraform Plan
        id: plan
        if: inputs.action == 'plan'
        # Runs in ./infra/terraform (job default)
        run: terraform plan

      - name: Terraform Apply
        if: inputs.action == 'apply'
        # Runs in ./infra/terraform (job default)
        run: terraform apply -auto-approve

      - name: Capture Terraform Outputs
        id: outputs
        if: inputs.action == 'apply'
        shell: bash
        run: |
          API_APP_NAME=$(terraform output -raw api_app_service_name 2>/dev/null || echo "")
          REACT_APP_NAME=$(terraform output -raw react_app_service_name 2>/dev/null || echo "")
          ACR_REGISTRY_URL=$(terraform output -raw acr_registry_url 2>/dev/null || echo "")
          RESOURCE_GROUP_NAME=$(terraform output -raw resource_group_name 2>/dev/null || echo "")

          echo "Captured Terraform Outputs:"
          echo "  API App Name: ${API_APP_NAME}"
          echo "  React App Name: ${REACT_APP_NAME}"
          echo "  ACR Registry URL: ${ACR_REGISTRY_URL}"
          echo "  Resource Group Name: ${RESOURCE_GROUP_NAME}"

          echo "api_app_name=${API_APP_NAME}" >> $GITHUB_OUTPUT
          echo "react_app_name=${REACT_APP_NAME}" >> $GITHUB_OUTPUT
          echo "acr_registry_url=${ACR_REGISTRY_URL}" >> $GITHUB_OUTPUT
          echo "resource_group_name=${RESOURCE_GROUP_NAME}" >> $GITHUB_OUTPUT

      - name: Terraform Destroy
        if: inputs.action == 'destroy'
        # Runs in ./infra/terraform (job default)
        run: |
          if [ "${{ inputs.environment }}" == "prod" ]; then
            echo "‚ùå ERROR: Terraform destroy is not allowed in production environment!"
            echo "This is a safety measure to prevent accidental deletion of production resources."
            exit 1
          fi
          echo "‚úÖ Proceeding with destroy for environment: ${{ inputs.environment }}"
          terraform destroy -auto-approve
