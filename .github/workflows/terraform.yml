# Add a short, numbered name for roadmap clarity
name: "02 - Terraform Infrastructure"
on:
  workflow_call:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        type: string
      action:
        description: 'Terraform action to perform'
        required: false
        type: string
        default: 'plan'
      allow_local_firewall:
        description: 'Allow local firewall rule for SQL Server (act testing only)'
        required: false
        type: boolean
        default: false
    outputs:
      api_app_name:
        description: "The name of the API App Service"
        value: ${{ jobs.terraform.outputs.api_app_name }}
      react_app_name:
        description: "The name of the React App Service"
        value: ${{ jobs.terraform.outputs.react_app_name }}
      acr_registry_url:
        description: "The URL of the Azure Container Registry"
        value: ${{ jobs.terraform.outputs.acr_registry_url }}
      resource_group_name:
        description: "The name of the Azure Resource Group"
        value: ${{ jobs.terraform.outputs.resource_group_name }}
    secrets:
      AZURE_CREDENTIALS:
        required: true
      AZURE_SUBSCRIPTION_ID:
        required: true
      SQL_ADMIN_USERNAME:
        required: true
      SQL_ADMIN_PASSWORD:
        required: true
      ARM_CLIENT_ID:
        required: true
      ARM_CLIENT_SECRET:
        required: true
      ARM_TENANT_ID:
        required: true


env:
  WORKING_DIRECTORY: './infra/terraform'

jobs:
  terraform:
    name: "Terraform apply"
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/your-github-username/consilientwebapp/actions-runner:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    environment: ${{ inputs.environment }}
    timeout-minutes: 60

    outputs:
      api_app_name: ${{ steps.outputs.outputs.api_app_name }}
      react_app_name: ${{ steps.outputs.outputs.react_app_name }}
      acr_registry_url: ${{ steps.outputs.outputs.acr_registry_url }}
      resource_group_name: ${{ steps.outputs.outputs.resource_group_name }}

    permissions:
      contents: read
      id-token: write

    concurrency:
      group: terraform-${{ inputs.environment }}
      cancel-in-progress: false

    env:
      ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
      # Terraform variables - all passed via environment to avoid committing .tfvars with secrets
      TF_VAR_environment: ${{ inputs.environment }}
      TF_VAR_subscription_id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      TF_VAR_sql_admin_username: ${{ vars.SQL_ADMIN_USERNAME || 'sqladmin' }}
      TF_VAR_sql_admin_password: ${{ secrets.SQL_ADMIN_PASSWORD }}
      TF_VAR_enable_local_firewall: ${{ inputs.allow_local_firewall }}
      # Non-sensitive configuration variables (from GitHub Variables)
      # Environment-specific (from GitHub Environment Variables)
      TF_VAR_region: ${{ vars.AZURE_REGION || 'canadacentral' }}
      TF_VAR_resource_group_name: ${{ vars.AZURE_RESOURCE_GROUP_NAME || 'consilient-resource-group' }}
      # Container App Environment configuration (from GitHub Repository Variables)
      TF_VAR_create_container_app_environment: ${{ vars.CAE_CREATE_NEW || 'false' }}
      TF_VAR_use_shared_container_environment: ${{ vars.CAE_USE_SHARED || 'true' }}
      TF_VAR_shared_container_environment_name: ${{ vars.CAE_SHARED_NAME || 'consilient-env' }}
      TF_VAR_container_app_environment_name_template: ${{ vars.CAE_NAME_TEMPLATE || 'consilient-cae-{environment}' }}

    defaults:
      run:
        working-directory: ${{ env.WORKING_DIRECTORY }}

    steps:
      - name: Validate Inputs
        working-directory: .
        shell: bash
        run: |
          # Validate action parameter
          if [[ ! "${{ inputs.action }}" =~ ^(plan|apply|destroy)$ ]]; then
            echo "‚ùå ERROR: Invalid action '${{ inputs.action }}'"
            echo "Valid actions are: plan, apply, destroy"
            exit 1
          fi

          echo "‚úÖ Input validation passed"
          echo "Action: ${{ inputs.action }}"
          echo "Environment: ${{ inputs.environment }}"

      - name: Checkout code
        uses: actions/checkout@v4.2.2

      - name: Login to Azure (OIDC + act fallback)
        uses: ./.github/actions/azure-login
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          azure-credentials: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Terraform Format Check
        id: fmt
        run: terraform fmt -check -recursive || [ -n "$ACT" ]

      - name: Debug Environment Variables
        shell: bash
        run: |
          echo "=== Terraform Environment Variables ==="
          echo "TF_VAR_environment: ${TF_VAR_environment}"
          echo "TF_VAR_subscription_id: ${TF_VAR_subscription_id:0:8}..."
          echo "TF_VAR_region: ${TF_VAR_region}"
          echo "TF_VAR_resource_group_name: ${TF_VAR_resource_group_name}"
          echo ""
          echo "=== Configuration Source ==="
          echo "AZURE_REGION (from vars): ${{ vars.AZURE_REGION || 'not set (using default)' }}"
          echo "AZURE_RESOURCE_GROUP_NAME (from vars): ${{ vars.AZURE_RESOURCE_GROUP_NAME || 'not set (using default)' }}"
          echo "CAE_CREATE_NEW (from vars): ${{ vars.CAE_CREATE_NEW || 'not set (using default)' }}"
          echo "CAE_USE_SHARED (from vars): ${{ vars.CAE_USE_SHARED || 'not set (using default)' }}"
          echo ""

          # Fail early if environment is empty
          if [ -z "${TF_VAR_environment}" ]; then
            echo "‚ùå ERROR: TF_VAR_environment is empty!"
            echo "When running with act, use: act workflow_dispatch --input environment=dev"
            exit 1
          fi

      - name: Configure Local Firewall Variable
        run: |
          # Override TF_VAR_enable_local_firewall based on ACT environment
          # ACT is set automatically by the act tool when running locally
          if [ -n "$ACT" ] && [ "${{ inputs.allow_local_firewall }}" = "true" ]; then
            echo "TF_VAR_enable_local_firewall=true" >> $GITHUB_ENV
            echo "‚úÖ Local firewall rule ENABLED (act detected)"
          else
            echo "TF_VAR_enable_local_firewall=false" >> $GITHUB_ENV
            echo "‚ÑπÔ∏è Local firewall rule disabled"
          fi

      - name: Terraform Init
        id: init
        run: terraform init

      - name: Build Resource Group ID
        shell: bash
        run: |
          echo ""
          echo "=== Building Resource Group ID ==="
          echo ""

          echo "Environment: ${TF_VAR_environment}"
          echo "Region: ${TF_VAR_region}"
          echo "Resource Group: ${TF_VAR_resource_group_name}"

          # Build RG_ID for later use in resource imports
          RG_ID="/subscriptions/${TF_VAR_subscription_id}/resourceGroups/${TF_VAR_resource_group_name}"
          echo "Resource Group ID: ${RG_ID}"
          echo "RG_ID=${RG_ID}" >> $GITHUB_ENV

      - name: Detect and Import Existing Resources
        id: import
        shell: bash
        run: |
          set +e  # Don't exit on error, we'll handle errors ourselves

          # Check if state file exists - skip imports if this is a fresh deployment
          if [ ! -f "terraform.tfstate" ] && [ ! -f ".terraform/terraform.tfstate" ]; then
            echo "‚ÑπÔ∏è  No Terraform state file found - this appears to be a fresh deployment"
            echo "‚ÑπÔ∏è  Skipping resource import step (will create all resources from scratch)"
            exit 0
          fi

          echo "=== Step 1: Reading CAE Configuration from Environment Variables ==="

          # CAE configuration from environment variables
          USE_SHARED_CAE="${TF_VAR_use_shared_container_environment}"
          SHARED_CAE_NAME="${TF_VAR_shared_container_environment_name}"
          CREATE_CAE="${TF_VAR_create_container_app_environment}"

          echo "Configuration from environment:"
          echo "  use_shared_container_environment: ${USE_SHARED_CAE}"
          echo "  shared_container_environment_name: ${SHARED_CAE_NAME}"
          echo "  create_container_app_environment: ${CREATE_CAE}"

          # Determine expected CAE name based on configuration
          if [ "$USE_SHARED_CAE" = "true" ]; then
            # Shared mode: Use fixed name (NO environment substitution!)
            EXPECTED_CAE_NAME="${SHARED_CAE_NAME}"
            echo "Using shared mode with FIXED name: ${EXPECTED_CAE_NAME}"
            echo "  (This name is used across ALL environments: dev, staging, prod)"
          else
            # Template mode: Would resolve from template, but we'll just let Terraform handle it
            echo "Using template mode - Terraform will resolve the name"
            EXPECTED_CAE_NAME=""  # Don't try to guess
          fi

          echo ""
          echo "‚úÖ Configuration loaded from environment variables"
          echo "All CAE settings will be managed by Terraform"

          echo ""
          echo "=== Step 2: Importing Existing Azure Resources ==="
          echo ""

          # Function to import a resource if it exists
          import_resource() {
            local tf_address="$1"
            local resource_id="$2"
            local resource_name="$3"

            # Check if already in state
            if terraform state list | grep -q "^${tf_address}\$"; then
              echo "  ‚úÖ Already in state: ${resource_name}"
              return 0
            fi

            # Try to import
            echo "  üì• Importing: ${resource_name}"
            local import_output
            if import_output=$(terraform import "${tf_address}" "${resource_id}" 2>&1); then
              echo "  ‚úÖ Imported successfully: ${resource_name}"
              return 0
            else
              # Check if error is because resource doesn't exist
              if echo "$import_output" | grep -q "Cannot import non-existent"; then
                echo "  ‚ÑπÔ∏è  Resource does not exist in Azure: ${resource_name}"
              elif echo "$import_output" | grep -q "Resource already managed"; then
                echo "  ‚úÖ Already managed: ${resource_name}"
              else
                echo "  ‚ö†Ô∏è  Import failed: ${resource_name}"
                echo "$import_output" | grep -i "error" | head -3
              fi
              return 1
            fi
          }

          echo "1. Resource Group"
          RG_ID="/subscriptions/${TF_VAR_subscription_id}/resourceGroups/${TF_VAR_resource_group_name}"
          import_resource "azurerm_resource_group.main" "${RG_ID}" "Resource Group"

          echo ""
          echo "2. Networking Resources"
          VNET_ID="${RG_ID}/providers/Microsoft.Network/virtualNetworks/consilient-vnet-${TF_VAR_environment}"
          SUBNET_ID="${VNET_ID}/subnets/consilient-subnet-${TF_VAR_environment}"
          import_resource "azurerm_virtual_network.main" "${VNET_ID}" "Virtual Network"
          import_resource "azurerm_subnet.main" "${SUBNET_ID}" "Subnet"

          echo ""
          echo "3. Container Registry"
          ACR_NAME=$(terraform output -raw acr_name 2>/dev/null || echo "consilientacr${TF_VAR_environment}$(echo -n "${TF_VAR_subscription_id}-${TF_VAR_resource_group_name}" | md5sum | cut -c1-6)")
          ACR_ID="${RG_ID}/providers/Microsoft.ContainerRegistry/registries/${ACR_NAME}"
          import_resource "azurerm_container_registry.main" "${ACR_ID}" "Container Registry"

          echo ""
          echo "4. SQL Server and Databases"
          SQL_SUFFIX=$(echo -n "${TF_VAR_subscription_id}-${TF_VAR_resource_group_name}" | md5sum | cut -c1-6)
          SQL_SERVER_NAME="consilient-sqlsrv-${TF_VAR_environment}-${SQL_SUFFIX}"
          SQL_SERVER_ID="${RG_ID}/providers/Microsoft.Sql/servers/${SQL_SERVER_NAME}"
          MAIN_DB_ID="${SQL_SERVER_ID}/databases/consilient_main_${TF_VAR_environment}"
          HANGFIRE_DB_ID="${SQL_SERVER_ID}/databases/consilient_hangfire_${TF_VAR_environment}"
          import_resource "azurerm_mssql_server.main" "${SQL_SERVER_ID}" "SQL Server"
          import_resource "module.main_db.azurerm_mssql_database.this" "${MAIN_DB_ID}" "Main Database"
          import_resource "module.hangfire_db.azurerm_mssql_database.this" "${HANGFIRE_DB_ID}" "Hangfire Database"

          echo ""
          echo "5. Storage Account and Container"
          STORAGE_NAME="consilientloki${TF_VAR_environment}${SQL_SUFFIX}"
          STORAGE_ID="${RG_ID}/providers/Microsoft.Storage/storageAccounts/${STORAGE_NAME}"
          STORAGE_CONTAINER_ID="${STORAGE_ID}/blobServices/default/containers/loki-data"
          PRIVATE_ENDPOINT_ID="${RG_ID}/providers/Microsoft.Network/privateEndpoints/consilient-pe-loki-storage-${TF_VAR_environment}"
          import_resource "azurerm_storage_account.loki" "${STORAGE_ID}" "Loki Storage Account"
          import_resource "azurerm_storage_container.loki" "${STORAGE_CONTAINER_ID}" "Loki Storage Container"
          import_resource "azurerm_private_endpoint.loki_storage" "${PRIVATE_ENDPOINT_ID}" "Loki Storage Private Endpoint"

          echo ""
          echo "6. Managed Identity"
          IDENTITY_ID="${RG_ID}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/consilient-loki-identity-${TF_VAR_environment}"
          import_resource "azurerm_user_assigned_identity.loki" "${IDENTITY_ID}" "Loki Managed Identity"

          echo ""
          echo "7. Role Assignment (Loki -> Storage)"
          # Get the principal_id of the managed identity
          PRINCIPAL_ID=$(az identity show --ids "${IDENTITY_ID}" --query principalId -o tsv 2>/dev/null || echo "")
          if [ -n "$PRINCIPAL_ID" ]; then
            echo "  Found managed identity principal ID: ${PRINCIPAL_ID}"
            # List role assignments for the storage account and find the one with this principal
            ROLE_ASSIGNMENT_ID=$(az role assignment list --scope "${STORAGE_ID}" --query "[?principalId=='${PRINCIPAL_ID}'].id | [0]" -o tsv 2>/dev/null || echo "")
            if [ -n "$ROLE_ASSIGNMENT_ID" ]; then
              echo "  Found role assignment: ${ROLE_ASSIGNMENT_ID}"
              import_resource "azurerm_role_assignment.loki_blob" "${ROLE_ASSIGNMENT_ID}" "Loki Storage Blob Role Assignment"
            else
              echo "  ‚ÑπÔ∏è  No role assignment found for this managed identity on storage account"
            fi
          else
            echo "  ‚ÑπÔ∏è  Managed identity not found, skipping role assignment import"
          fi

          echo ""
          echo "8. Container App Environment"
          # Import CAE only if:
          # 1. We're NOT creating a new one (create_container_app_environment = false)
          # 2. We're in shared mode (use_shared_container_environment = true)
          # 3. The CAE name is known (from terraform.tfvars)
          # 4. The CAE is in the SAME resource group as other resources

          if [ "$CREATE_CAE" = "false" ] && [ "$USE_SHARED_CAE" = "true" ] && [ -n "$EXPECTED_CAE_NAME" ]; then
            echo "  Shared mode detected: Checking if CAE '${EXPECTED_CAE_NAME}' exists in resource group '${TF_VAR_resource_group_name}'"

            # Check if CAE exists in Azure in the SAME resource group
            CAE_EXISTS=$(az containerapp env show \
              --name "${EXPECTED_CAE_NAME}" \
              --resource-group "${TF_VAR_resource_group_name}" \
              --query "id" -o tsv 2>/dev/null || echo "")

            if [ -n "$CAE_EXISTS" ]; then
              echo "  CAE exists in same resource group - attempting import"
              CAE_ID="${RG_ID}/providers/Microsoft.App/managedEnvironments/${EXPECTED_CAE_NAME}"
              import_resource "azurerm_container_app_environment.shared[0]" "${CAE_ID}" "Container App Environment"
            else
              echo "  ‚ÑπÔ∏è  CAE '${EXPECTED_CAE_NAME}' not found in resource group '${TF_VAR_resource_group_name}'"
              echo "  ‚ÑπÔ∏è  Terraform will use data source to reference CAE from another resource group"
            fi
          elif [ "$CREATE_CAE" = "true" ]; then
            echo "  ‚ÑπÔ∏è  create_container_app_environment=true - Terraform will create a new CAE"
            echo "  ‚ÑπÔ∏è  Attempting to import in case it already exists..."

            # Try to determine CAE name (in template mode, Terraform resolves it)
            if [ "$USE_SHARED_CAE" = "true" ]; then
              CAE_NAME="${EXPECTED_CAE_NAME}"
            else
              # Template mode - resolve the template from environment variable
              CAE_TEMPLATE="${TF_VAR_container_app_environment_name_template}"
              CAE_NAME="${CAE_TEMPLATE/\{environment\}/${TF_VAR_environment}}"
            fi

            if [ -n "$CAE_NAME" ]; then
              CAE_ID="${RG_ID}/providers/Microsoft.App/managedEnvironments/${CAE_NAME}"
              import_resource "azurerm_container_app_environment.shared[0]" "${CAE_ID}" "Container App Environment"
            fi
          else
            echo "  ‚ÑπÔ∏è  CAE configuration doesn't require import (using existing CAE via data source or ID)"
          fi

          echo ""
          echo "9. Container App (Loki)"
          CONTAINER_APP_ID="${RG_ID}/providers/Microsoft.App/containerApps/consilient-loki-${TF_VAR_environment}"
          import_resource "azurerm_container_app.loki" "${CONTAINER_APP_ID}" "Loki Container App"

          echo ""
          echo "10. Grafana"
          GRAFANA_ID="${RG_ID}/providers/Microsoft.Dashboard/grafana/consilient-grafana-${TF_VAR_environment}"
          import_resource "azurerm_dashboard_grafana.main" "${GRAFANA_ID}" "Grafana Dashboard"

          echo ""
          echo "11. App Service Plans"
          ASP_REACT_ID="${RG_ID}/providers/Microsoft.Web/serverFarms/consilient-asp-react-${TF_VAR_environment}"
          ASP_API_ID="${RG_ID}/providers/Microsoft.Web/serverFarms/consilient-asp-api-${TF_VAR_environment}"
          import_resource "module.react_app.azurerm_service_plan.this" "${ASP_REACT_ID}" "React App Service Plan"
          import_resource "module.api_app.azurerm_service_plan.this" "${ASP_API_ID}" "API App Service Plan"

          echo ""
          echo "12. App Services"
          APP_REACT_ID="${RG_ID}/providers/Microsoft.Web/sites/consilient-react-${TF_VAR_environment}"
          APP_API_ID="${RG_ID}/providers/Microsoft.Web/sites/consilient-api-${TF_VAR_environment}"
          import_resource "module.react_app.azurerm_linux_web_app.this" "${APP_REACT_ID}" "React App Service"
          import_resource "module.api_app.azurerm_linux_web_app.this" "${APP_API_ID}" "API App Service"

          echo ""
          echo "=== Import Process Complete ==="
          echo ""
          echo "Configuration will be managed by terraform.tfvars"
          echo "No workflow overrides applied"

          exit 0  # Always succeed, imports are best-effort

      - name: Terraform Validate
        id: validate
        run: terraform validate

      - name: Terraform Plan
        id: plan
        if: inputs.action == 'plan'
        run: terraform plan

      - name: Terraform Apply
        if: inputs.action == 'apply'
        run: terraform apply -auto-approve

      - name: Capture Terraform Outputs
        id: outputs
        if: inputs.action == 'apply'
        shell: bash
        run: |
          API_APP_NAME=$(terraform output -raw api_app_service_name 2>/dev/null || echo "")
          REACT_APP_NAME=$(terraform output -raw react_app_service_name 2>/dev/null || echo "")
          ACR_REGISTRY_URL=$(terraform output -raw acr_registry_url 2>/dev/null || echo "")
          RESOURCE_GROUP_NAME=$(terraform output -raw resource_group_name 2>/dev/null || echo "")

          echo "Captured Terraform Outputs:"
          echo "  API App Name: ${API_APP_NAME}"
          echo "  React App Name: ${REACT_APP_NAME}"
          echo "  ACR Registry URL: ${ACR_REGISTRY_URL}"
          echo "  Resource Group Name: ${RESOURCE_GROUP_NAME}"

          echo "api_app_name=${API_APP_NAME}" >> $GITHUB_OUTPUT
          echo "react_app_name=${REACT_APP_NAME}" >> $GITHUB_OUTPUT
          echo "acr_registry_url=${ACR_REGISTRY_URL}" >> $GITHUB_OUTPUT
          echo "resource_group_name=${RESOURCE_GROUP_NAME}" >> $GITHUB_OUTPUT

      - name: Terraform Destroy
        if: inputs.action == 'destroy'
        run: |
          if [ "${{ inputs.environment }}" == "prod" ]; then
            echo "‚ùå ERROR: Terraform destroy is not allowed in production environment!"
            echo "This is a safety measure to prevent accidental deletion of production resources."
            exit 1
          fi
          echo "‚úÖ Proceeding with destroy for environment: ${{ inputs.environment }}"
          terraform destroy -auto-approve
